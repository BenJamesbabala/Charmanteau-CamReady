{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This code has been taken from https://github.com/mikesj-public/rnn_spelling_bee\n",
    "We have adapted it to current version of tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../Data/\"\n",
    "data_file = \"finalPorts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open(data_dir+data_file,\"r\").readlines()\n",
    "data = [row.strip().split(',') for row in data]\n",
    "data = [ [val for val in row if len(val)>0] for row in data ]\n",
    "labels = [row[0] for row in data]\n",
    "inp = [row[1:] for row in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n",
      "['animation', 'electronics']\n",
      "animatronics\n",
      "DescribeResult(nobs=1570, minmax=(1, 5), mean=2.0414012738853504, variance=0.067755842700734384, skewness=6.639999692867517, kurtosis=47.772364930390026)\n",
      "Counter({2: 1523, 3: 27, 4: 18, 1: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "# Few basic stats on data\n",
    "print len(inp)\n",
    "print inp[0]\n",
    "print labels[0]\n",
    "all_lengths = np.array( [len(x) for x in inp] )\n",
    "print stats.describe(all_lengths)\n",
    "print Counter(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animation electronics\n",
      "animatronics\n",
      "Counter({15: 202, 14: 194, 13: 162, 12: 160, 16: 147, 17: 115, 11: 107, 18: 89, 10: 84, 19: 74, 20: 61, 9: 42, 21: 40, 22: 33, 23: 15, 8: 13, 24: 8, 25: 6, 7: 5, 26: 5, 27: 4, 28: 2, 30: 1, 31: 1})\n",
      "max_x =  31\n",
      "Counter({15: 1570})\n",
      "max_y =  17\n",
      "\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Merge indiivdual words in input with a space\n",
    "X = [ ' '.join(row) for row in inp ]\n",
    "#X = [ list(x) for x in X ]\n",
    "Y = labels\n",
    "#Y = [ list(y) for y in Y ]\n",
    "print X[0]\n",
    "print Y[0]\n",
    "print Counter([len(x) for x in X])\n",
    "max_x = max( [len(x) for x in X] )\n",
    "print \"max_x = \",max_x\n",
    "print Counter([len(x) for y in Y])\n",
    "max_y = max( [len(y) for y in Y] )\n",
    "print \"max_y = \",max_y\n",
    "\n",
    "all_chars_set = set([])\n",
    "all_chars_set.add('_') #'_' will be padding symbol\n",
    "for x in X:\n",
    "    for x_char in x:\n",
    "        all_chars_set.add(x_char)\n",
    "print \"\"\n",
    "print len(all_chars_set)\n",
    "## Note we have not taken care of order of words in input - ideally we would like to shuffle the word orders in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '_', 2: 'a', 3: 'c', 4: 'b', 5: 'e', 6: 'd', 7: 'g', 8: 'f', 9: 'i', 10: 'h', 11: 'k', 12: 'j', 13: 'm', 14: 'l', 15: 'o', 16: 'n', 17: 'q', 18: 'p', 19: 's', 20: 'r', 21: 'u', 22: 't', 23: 'w', 24: 'v', 25: 'y', 26: 'x', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "index_to_letter = dict(enumerate(list(all_chars_set)))\n",
    "letter_to_index = dict((v, k) for k,v in index_to_letter.items())\n",
    "print index_to_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncmu_dict_raw = open(\"cmudict-0.7b\").read()\\n\\nfirst_line = \"A  AH0\"\\nlast_line = \"ZYWICKI  Z IH0 W IH1 K IY0\"\\n\\nlines = cmu_dict_raw.split(\"\\n\")\\n\\nfor i, l in enumerate(lines):\\n    if l == first_line:\\n        first_index = i\\n    if l == last_line:\\n        last_index = i\\n        \\nprint \"Example line of file : \"\\nprint lines[113108]\\n\\n#-----------------------------\\n\\nphonemes = set()\\n\\nfor l in lines[first_index : last_index + 1]:\\n    word, pronounce = l.split(\"  \")\\n    for phoneme in pronounce.split():\\n        phonemes.add(phoneme)\\n        \\nsorted_phonemes = [\"_\"] + sorted(list(phonemes))\\n\\nindex_to_phoneme = dict(enumerate(sorted_phonemes))\\nphoneme_to_index = dict((v, k) for k,v in index_to_phoneme.items())\\n\\nindex_to_letter = dict(enumerate(\"_abcdefghijklmnopqrstuvwxyz\"))\\nletter_to_index = dict((v, k) for k,v in index_to_letter.items())\\n\\n#--------------------------------\\n\\nfrom collections import defaultdict\\n\\npronounce_dict = {}\\n\\nfor l in lines[first_index : last_index + 1]:\\n    word, phone_list = l.split(\"  \")\\n    pronounce_dict[word.lower()] = [phoneme_to_index[p] for p in phone_list.split()]\\n    \\n#--------------------------------\\nmax_k = max([len(k) for k,v in pronounce_dict.items()])\\nmax_v = max([len(v) for k,v in pronounce_dict.items()])\\nfor k,v in pronounce_dict.items():\\n    if len(k) == max_k or  len(v) == max_v:\\n        print k\\n        print v\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cmu_dict_raw = open(\"cmudict-0.7b\").read()\n",
    "\n",
    "first_line = \"A  AH0\"\n",
    "last_line = \"ZYWICKI  Z IH0 W IH1 K IY0\"\n",
    "\n",
    "lines = cmu_dict_raw.split(\"\\n\")\n",
    "\n",
    "for i, l in enumerate(lines):\n",
    "    if l == first_line:\n",
    "        first_index = i\n",
    "    if l == last_line:\n",
    "        last_index = i\n",
    "        \n",
    "print \"Example line of file : \"\n",
    "print lines[113108]\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "phonemes = set()\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, pronounce = l.split(\"  \")\n",
    "    for phoneme in pronounce.split():\n",
    "        phonemes.add(phoneme)\n",
    "        \n",
    "sorted_phonemes = [\"_\"] + sorted(list(phonemes))\n",
    "\n",
    "index_to_phoneme = dict(enumerate(sorted_phonemes))\n",
    "phoneme_to_index = dict((v, k) for k,v in index_to_phoneme.items())\n",
    "\n",
    "index_to_letter = dict(enumerate(\"_abcdefghijklmnopqrstuvwxyz\"))\n",
    "letter_to_index = dict((v, k) for k,v in index_to_letter.items())\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "pronounce_dict = {}\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, phone_list = l.split(\"  \")\n",
    "    pronounce_dict[word.lower()] = [phoneme_to_index[p] for p in phone_list.split()]\n",
    "    \n",
    "#--------------------------------\n",
    "max_k = max([len(k) for k,v in pronounce_dict.items()])\n",
    "max_v = max([len(v) for k,v in pronounce_dict.items()])\n",
    "for k,v in pronounce_dict.items():\n",
    "    if len(k) == max_k or  len(v) == max_v:\n",
    "        print k\n",
    "        print v\n",
    "#--------------------------------\n",
    "bad_ct = set()\n",
    "\n",
    "letters = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "print len(pronounce_dict)\n",
    "for k, v in pronounce_dict.items():\n",
    "    if len(k) < 5 or len(k) > 15:\n",
    "        del pronounce_dict[k]\n",
    "        continue\n",
    "    for c in k:\n",
    "        if c not in letters:\n",
    "            del pronounce_dict[k]\n",
    "            break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b = a+[4,5]\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133779\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split lines into words, phonemes, convert to indexes (with padding), split into training, validation, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animation electronics\n",
      "[ 2 16  9 13  2 22  9 15 16  0  5 14  5  3 22 20 15 16  9  3 19  1  1  1  1\n",
      "  1  1  1  1  1  1]\n",
      "animatronics\n",
      "[ 2 16  9 13  2 22 20 15 16  9  3 19  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_ = np.zeros((len(X), max_x))\n",
    "labels_ = np.zeros((len(Y), max_y)) # len(Y) should be same as len(X)\n",
    "\n",
    "i=0\n",
    "for x,y in zip(X,Y):\n",
    "    tmpx = x + \"_\" * ( max_x - len(x) )\n",
    "    tmpy = y + \"_\" * ( max_y - len(y))\n",
    "    for j,letter in enumerate(tmpx):\n",
    "        input_[i][j] = letter_to_index[letter]\n",
    "    for j,letter in enumerate(tmpy):\n",
    "        labels_[i][j] = letter_to_index[letter]\n",
    "    i+=1\n",
    "        \n",
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "print X[0]\n",
    "print input_[0]\n",
    "print Y[0]\n",
    "print labels_[0]\n",
    "\n",
    "\n",
    "input_test   = input_[:1000]\n",
    "input_val    = input_[1000:1200]\n",
    "input_train  = input_[1200:]\n",
    "labels_test  = labels_[:1000]\n",
    "labels_val   = labels_[1000:1200]\n",
    "labels_train = labels_[1200:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorflow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#import tensorflow.nn.rnn_cell\n",
    "#from tensorflow.models import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell resets the graphs and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    \n",
    "    pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_length = max_x\n",
    "output_seq_length = max_y\n",
    "batch_size = 28\n",
    "\n",
    "input_vocab_size = len(letter_to_index)\n",
    "output_vocab_size = len(letter_to_index)\n",
    "embedding_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on this page we take our Seq2Seq learner to have the follwing shape:\n",
    "\n",
    "![alt text](https://www.tensorflow.org/versions/r0.7/images/basic_seq2seq.png \"Seq2Seq\")\n",
    "\n",
    "This means the decode_input has to be shifted along by one from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, \n",
    "                                shape=(None,),\n",
    "                                name = \"ei_%i\" %i)\n",
    "                                for i in range(input_seq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32,\n",
    "                                shape=(None,),\n",
    "                                name = \"l_%i\" %i)\n",
    "                                for i in range(output_seq_length)]\n",
    "\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the meat of the model, and a lot is happening here under the hood.  We take our cells to be LSTM recurrent units, with dropout between the feed-forward layers.  We take 3 of these stacked as our neural network.  We then run this using the seq2seq.embedding_rnn_seq2seq pattern - this let's us hand the neural network sequences like 1,2,3,2,1 - and the neural network automatically embeds this as a one-hot tensor for us.  \n",
    "\n",
    "Note that we build two networks within the 'decoders' scope.  One of these is using feed_previous = True, the other not.  We set this to False during training, so that even if the learner makes a mistake on a letter - we still give it the correct label in the decoder_inputs.  Since we don't have the real label for the test set, this is set to True, and the decoder takes the letter with maximum probability from the last step of the decoder output.  \n",
    "\n",
    "The decode_output is a tensor of shape (batch_size, output_vocab_size).  We can run softmax on this to get logit scores for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(embedding_dim), output_keep_prob=keep_prob\n",
    "    ) for i in range(3)]\n",
    "\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, embedding_dim)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, embedding_dim, \n",
    "        feed_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence_loss is cross-entropy on the soft max of the decode outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = tf.nn.seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-9f5df2c7784a>:1 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple class for getting random batches and reshaping them properly for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = self.iter.next()\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = self.iter.next()\n",
    "        X, Y = zip(*[self.data[i] for i in idxs])\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)\n",
    "    \n",
    "train_iter = DataIterator(data_train, batch_size)\n",
    "val_iter = DataIterator(data_val, batch_size)\n",
    "test_iter = DataIterator(data_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation scores are based on the seq2seq loss, and on the precision - the number of words that the model spells perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(input_seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(output_seq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simply ran this until it looked like the validation set score was not improving then aborted with a keyboard interrupt.  This took about 20 minutes on my Titan X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss   : 1.876281, val predict   = 0.0%\n",
      "train loss : 1.900723, train predict = 0.0%\n",
      "\n",
      "val loss   : 1.796410, val predict   = 0.0%\n",
      "train loss : 1.829361, train predict = 0.0%\n",
      "\n",
      "val loss   : 1.757640, val predict   = 0.0%\n",
      "train loss : 1.797871, train predict = 0.0%\n",
      "\n",
      "val loss   : 1.724230, val predict   = 0.0%\n",
      "train loss : 1.765082, train predict = 0.0%\n",
      "\n",
      "val loss   : 1.710278, val predict   = 0.0%\n",
      "train loss : 1.759651, train predict = 0.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 16)\n",
    "            print \"val loss   : %f, val predict   = %.1f%%\" %(val_loss, val_predict * 100)\n",
    "            print \"train loss : %f, train predict = %.1f%%\" %(train_loss, train_predict * 100)\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print \"interrupted by user\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach ~ 50% on our hold-out validation set, which may seem low.  Let's see some of the outputs on our test data set, to see the kind of mistakes that the model is making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_loss, output, X, Y = get_eval_batch_data(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pronunciation                            real spelling     model spelling    is correct\n",
      "\n",
      "D-EH1-P-R-AH0-K-EY2-T-IH0-NG             deprecating       deeeeeiiinn       False\n",
      "K-ER1-L-AE0-K                            kurlak            ccckkk            False\n",
      "AE1-B-IH0-L-ER0-D                        abelard           bbbblee           False\n",
      "K-AO1-R-AH0-N                            coren             cronn             False\n",
      "P-AH0-T-R-OW0-L-M-EH1-N                  patrolmen         pprraaann         False\n",
      "W-AY1-K-M-AH0-N                          weichman          mmminn            False\n",
      "M-AH2-L-T-IY0-F-AH1-NG-K-SH-AH2-N-AH0-L  multifunctional   mmllliiiiiitttn   False\n",
      "L-UW1-M-ER0                              loomer            mlllrr            False\n",
      "JH-IY1-W-EH1-N                           jiawen            weeenn            False\n",
      "P-UH1-L-IH0-NG                           pulling           ppllnn            False\n",
      "AO1-R-T-N-ER0                            ortner            rrrrrr            False\n",
      "V-IY2-L-AA0-S-EH0-N-AO1-R                villasenor        llvaaeees         False\n",
      "AE1-M-B-R-IH0-T                          ambrit            baaattt           False\n",
      "HH-AW1-AA0-R-TH                          howarth           hhhhhhrr          False\n",
      "P-OW2-P-IY1-L                            popeil            pppppe            False\n",
      "S-AH0-B-K-AA1-N-T-IH0-N-AH0-N-T-S        subcontinents     sssootnniiiin     False\n",
      "T-OY1-F-AH0-L                            teufel            fftlll            False\n",
      "D-EY1-N-Y-AH0                            dania             ddann             False\n",
      "S-K-OW1-V-AH0-L                          scovel            soolll            False\n",
      "JH-AE1-G-W-AA2-R-Z                       jaguars           ggggrrs           False\n",
      "AH0-S-EH1-R-B-IH0-K                      acerbic           srrrecck          False\n",
      "P-OW1-S-T-M-AH0                          postma            ppoooa            False\n",
      "N-OW1-P                                  knope             poon              False\n",
      "W-AA1-D-AH0-Z                            wadas             aaaad             False\n",
      "AH0-N-S-UW1-P-ER0-V-AY2-Z-D              unsupervised      snnnneeeeee       False\n",
      "F-AE1-K-L-ER0                            fackler           ffllrr            False\n",
      "T-R-EH1-N-EH0-R-IY0                      trenary           rrrrree           False\n",
      "S-AE1-D-AH0-N-Z                          saddens           saasss            False\n",
      "HH-IH1-L-D-IH0-B-R-AH0-N                 hildebran         ddliiiin          False\n",
      "T-AO0-R-S-IY0-EH1-L-OW0                  torsiello         rrrttlll          False\n",
      "AA1-G-W-IY0-ER0                          aguiar            gggrrr            False\n",
      "D-ER1-TH                                 dearth            drrte             False\n",
      "B-R-AY1-N-Z                              brines            brins             False\n",
      "S-T-AE2-L-IH0-N-IH0-Z-EY1-SH-AH0-N       stalinization     sssttiiiinnnn     False\n",
      "L-IH2-P-OW0-P-R-OW1-T-IY0-N-Z            lipoproteins      ppppoooiiins      False\n",
      "AH1-S-ER0-IY0                            ussery            ssuer             False\n",
      "D-IH0-B-AH1-NG-K-S                       debunks           dbbiiis           False\n",
      "N-EH2-TH-UW1-V-AH0                       nethuva           nnnttt            False\n",
      "AH0-T-EH1-N-T-IH0-V-L-IY0                attentively       tttttiiill        False\n",
      "K-AE1-M-ER0                              kammer            mmmmr             False\n",
      "R-OW1-D-K-AE2-P                          roadcap           rrooaa            False\n",
      "IH0-F-EH1-K-T                            effect            feettt            False\n",
      "AH0-N-AE1-K-R-AH0-N-IH2-Z-AH0-M-Z        anachronisms      nnnnnnrrss        False\n",
      "T-AY1-M-Z                                times             mttss             False\n",
      "M-IH0-N-AO1-R-K-OW0                      minorco           mmooooo           False\n",
      "B-EY1-K-IH0-NG                           baking            bbbing            False\n",
      "G-AE1-L-IH0-P-AO2-L-T                    galipault         llllllttt         False\n",
      "HH-AH1-K-AH0-B-AH0                       huckaba           hhoooa            False\n",
      "B-L-IH1-T-S-IH0-Z                        blitzes           bblisss           False\n",
      "M-OW1-T-ER0-S-AY2-K-L-IH0-S-T-S          motorcyclists     mmttttttissss     False\n",
      "AO0-R-EH1-F-AY0-S                        orefice           frrrsss           False\n",
      "M-EH1-N-AH0-N-AY2-T-S                    mennonites        mnnnettt          False\n",
      "M-ER1-TH-AH0                             murtha            mmmtt             False\n",
      "D-IH1-L-K-S                              dilks             ddiis             False\n",
      "M-AA1-L-ER0                              mahler            mmllr             False\n",
      "IY1-R-F-AH2-L                            earful            fffell            False\n",
      "V-AA1-M-OW0-Z                            vamos             oooss             False\n",
      "CH-AY1-L-D-L-AH0-S                       childless         llllllss          False\n",
      "S-AH0-L-EH1-K-T-IH0-NG                   selecting         sslliiinn         False\n",
      "L-AA1-R-K                                larke             caale             False\n",
      "F-R-IY1-JH                               freije            ffeee             False\n",
      "JH-OW1-L-S-AH0-N                         joelson           lllonn            False\n",
      "M-AH0-K-EY1                              mccay             mmaaa             False\n",
      "EH1-N-IY0-TH-IH2-NG                      anything          nnniigg           False\n",
      "K-OW1-L-D-ER0-AH0-N                      coldren           ooooonnn          False\n",
      "P-OW1-L-IY0-OW2                          polio             poooo             False\n",
      "P-IH1-D-AH0-L-IH0-NG                     piddling          ppiiiinn          False\n",
      "IH0-N-G-EY1-JH-D                         engaged           gggggddd          False\n",
      "P-EH1-N-IY0                              pennie            ppenn             False\n",
      "S-IH1-NG-G-AH0-L-T-AH0-N                 singleton         siiinnnnn         False\n",
      "B-AO1-R-G-M-AY0-ER0                      borgmeyer         bbrrrrrr          False\n",
      "B-AY2-D-OW1-AH0                          baidoa            bbbdd             False\n",
      "S-AA0-N-Y-UW0-AA1-N                      sanjuan           ssnnnn            False\n",
      "Z-B-IH0-K-AW1-S-K-IY0                    zbikowski         bbbckkkk          False\n",
      "AE2-M-F-AH0-F-IH1-L-IH0-K                amphophilic       ffffliiii         False\n",
      "N-ER1-D-Z                                nerds             nneee             False\n",
      "T-AE1-NG-G-AH0-L                         tangle            gaaaal            False\n",
      "EH1-D-IY0-Z                              eddies            deees             False\n",
      "HH-AE1-N-D-AH0-D                         handed            aadddd            False\n",
      "B-AE1-K-F-IY2-L-D                        backfield         bbbblldd          False\n",
      "AH0-P-R-UW1-T-IH0-D                      uprooted          pprtttd           False\n",
      "P-EH1-R-AH0-L-AH0-S                      perilous          prrrees           False\n",
      "ER1-D-IY0-EY0-L-Z                        urdiales          ddiiiss           False\n",
      "R-EH1-D-ER0                              redder            rrrrr             False\n",
      "L-AA1-P-IH0-NG                           lopping           llllnn            False\n",
      "K-EY1-Z-ER0                              kaser             caeee             False\n",
      "K-IH1-R-AH0-K-OW2-F                      kiracofe          ccccccck          False\n",
      "B-ER1-N-EH1-T                            burnette          betttte           False\n",
      "AA0-R-N-OW1-L-D-IY0                      arnoldi           rronlll           False\n",
      "B-ER1-TH-M-AA2-R-K                       birthmark         bbrrrrrc          False\n",
      "L-IH0-L-IY0-AE1-N-AH0                    lilliana          llllnnn           False\n",
      "B-EY1-S-IH0-S-T                          bassist           bbbsss            False\n",
      "R-IH1-JH-L-IY0                           ridgely           riilll            False\n",
      "K-EH1-G-L-ER0                            kegler            eeeeerr           False\n",
      "F-L-AO1-R-IY0                            flory             fffrl             False\n",
      "IH2-N-F-Y-UW1-ZH-AH0-N                   infusion          nnnnni            False\n",
      "W-OW1-L-R-AE2-TH                         wolrath           haaaaae           False\n",
      "M-AA0-R-K-EH1-S-IY0                      marchesi          mmrrcc            False\n",
      "T-AE1-K-S-IY0-K-AE2-B                    taxicab           caccccckk         False\n",
      "F-OW1-N-Z                                phones            foon              False\n",
      "M-AH0-K-F-AE1-D-AH0-N                    mcfadden          maaaaann          False\n",
      "S-T-AA1-K-S                              stocks            sssss             False\n",
      "R-IH1-Z-L-IY0                            risley            rrilll            False\n",
      "S-W-AO1-NG-K                             swanke            ssonn             False\n",
      "P-UW0-CH-IH1-L-OW0                       pucillo           pooolll           False\n",
      "R-IH1-V-ER0-B-EH2-N-D                    riverbend         rrrreennn         False\n",
      "K-AE1-R-OW0                              carrow            caaa              False\n",
      "HH-EH1-L-M-AH0-T                         helmet            hleeett           False\n",
      "P-AA1-R-V-IH0-N                          parvin            pprnnn            False\n",
      "M-AA0-T-AA0-R-AA1-Z-OW0                  matarazzo         maaaaass          False\n",
      "K-AO2-R-EH1-L                            corell            cceell            False\n",
      "M-AY1-K-AH0-L-AY0-N                      michaeline        mmiiiinn          False\n",
      "K-OW1-CH-IH0-NG                          coaching          ccccnn            False\n",
      "B-IH1-R-D-S-T-AW2-N                      beardstown        bbrriinnn         False\n",
      "AE1-L-IY0-AH0-N-Z                        allianz           aaaanns           False\n",
      "K-L-IY1-N-L-IY0                          cleanly           lllllle           False\n",
      "B-IH1-L-IY0-T-ER0                        billeter          bbbiieee          False\n",
      "R-IY0-S-AY1-K-AH0-L-Z                    recycles          rrriiiss          False\n",
      "S-IH0-R-EH1-N-AH0                        sirena            seennn            False\n",
      "Y-UW1-K-ER0-D                            euchred           cooeee            False\n",
      "JH-AO0-R-D-AA1-N-OW0                     giordano          rrooonn           False\n",
      "S-P-AO1-R                                spore             pprr              False\n",
      "Y-ER1-N-Z                                yearns            nnee              False\n",
      "P-IY0-T-S-IY0-M-EH1-N-T-IY0              pizzimenti        pttttiiiin        False\n",
      "AA0-CH-EH1-T-AH0                         accetta           aatttt            False\n",
      "B-AE1-TH-R-UW2-M-Z                       bathrooms         bbbarrr           False\n",
      "P-AO0-R-T-K-AH1-L-IH0-S                  portculis         prrrtiiis         False\n",
      "R-AH0-F-AY1-N-M-AH0-N-T                  refinement        rrrrennnn         False\n"
     ]
    }
   ],
   "source": [
    "print \"pronunciation\".ljust(40),\n",
    "print \"real spelling\".ljust(17),\n",
    "print \"model spelling\".ljust(17),\n",
    "print \"is correct\"\n",
    "print\n",
    "\n",
    "for index in range(len(output)):\n",
    "    phonemes = \"-\".join([index_to_phoneme[p] for p in X.T[index]]) \n",
    "    real = [index_to_letter[l] for l in Y.T[index]] \n",
    "    predict = [index_to_letter[l] for l in np.argmax(output, axis = 2)[index]]\n",
    "   \n",
    "    print phonemes.split(\"-_\")[0].ljust(40),\n",
    "    print \"\".join(real).split(\"_\")[0].ljust(17),\n",
    "    print \"\".join(predict).split(\"_\")[0].ljust(17),\n",
    "    print str(real == predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
